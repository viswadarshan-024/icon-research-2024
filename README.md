# Enhancing Masked Word Prediction in Tamil Language Models  
### A Synergistic Approach Using BERT and SBERT  

[ACL Anthology](https://aclanthology.org/2024.icon-1.55/)  

## Paper Published at ACL Anthology (ICON 2024)  
**"Enhancing Masked Word Prediction in Tamil Language Models: A Synergistic Approach Using BERT and SBERT"**  

ðŸ”— **Publication Link:** [ACL Anthology](https://aclanthology.org/2024.icon-1.55/)  

## Overview  
Masked word prediction plays a crucial role in **Tamil NLP**, especially in improving the contextual understanding of **low-resource languages**. This work proposes a novel **hybrid approach** combining **BERT** and **SBERT**, achieving enhanced semantic representation and improved performance in masked token prediction tasks for Tamil.  

**Check out the model in Hugging Face:** [Tamil MLM](https://huggingface.co/viswadarshan06/Tamil-MLM)

## Key Contributions  
- **Hybrid BERT-SBERT Model**: We leverage both **BERT** for contextual embeddings and **SBERT** for semantic similarity refinement.  
- **Improved Masked Word Prediction**: Our method outperforms standard BERT-based approaches in **Tamil** text completion tasks.  
- **Low-Resource Language Adaptation**: Focused improvements for **Tamil**, with techniques adaptable to other Indic languages.  
- **Extensive Evaluation**: Benchmarked on **Oscar Corpus Tamil**, with comprehensive results demonstrating enhanced **accuracy** and **semantic coherence**.


## Citation
```
@inproceedings{yourcitation,
  author = {Viswadarshan R R, Viswaa Selvam S, Felcia Lilian J, Mahalakshmi S},
  title = {Enhancing Masked Word Prediction in Tamil Language Models: A Synergistic Approach Using BERT and SBERT},
  booktitle = {Proceedings of ICON 2024},
  year = {2024},
  url = {https://aclanthology.org/2024.icon-1.55/}
}
```

